{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM example\n",
    "#### Resource: https://www.udacity.com/course/computer-vision-nanodegree--nd891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x102add250>"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# so that random variables will be consistent and repeatable for testing\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      " [tensor([[1.4934, 0.4987, 0.2319, 1.1746]]), tensor([[-1.3967,  0.8998,  1.0956, -0.5231]]), tensor([[-0.8462, -0.9946,  0.6311,  0.5327]]), tensor([[-0.8454,  0.9406, -2.1224,  0.0233]]), tensor([[ 0.4836,  1.2895,  0.8957, -0.2465]])]\n",
      "\n",
      "\n",
      "Initial hidden state tensor([[[ 1.9422, -0.3628, -1.0494]]])\n",
      "Initial cell memory tensor([[[-1.0264,  1.3494,  0.8018]]])\n",
      "\n",
      "out: \n",
      " tensor([[[-0.4372,  0.2583,  0.2947]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.4372,  0.2583,  0.2947]]], grad_fn=<StackBackward>), tensor([[[-0.7344,  0.6209,  0.4191]]], grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[-0.2836,  0.1314,  0.4133]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.2836,  0.1314,  0.4133]]], grad_fn=<StackBackward>), tensor([[[-0.5041,  0.2672,  0.6370]]], grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[-0.3404,  0.4880,  0.1949]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.3404,  0.4880,  0.1949]]], grad_fn=<StackBackward>), tensor([[[-0.5552,  0.7909,  0.3300]]], grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[-0.3544,  0.2405,  0.3150]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.3544,  0.2405,  0.3150]]], grad_fn=<StackBackward>), tensor([[[-0.5645,  1.0073,  0.6101]]], grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[-0.3328,  0.0437,  0.3817]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.3328,  0.0437,  0.3817]]], grad_fn=<StackBackward>), tensor([[[-0.5311,  0.1181,  0.5304]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# define an LSTM with an input dim of 4 and hidden dim of 3\n",
    "# this expects to see 4 values as input and generates 3 values as output\n",
    "input_dim = 4\n",
    "hidden_dim = 3\n",
    "lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim)  \n",
    "\n",
    "# make 5 input sequences of 4 random values each\n",
    "inputs_list = [torch.randn(1, input_dim) for _ in range(5)]\n",
    "print('inputs: \\n', inputs_list)\n",
    "print('\\n')\n",
    "\n",
    "# initialize the hidden state\n",
    "# (1 layer, 1 batch_size, 3 outputs)\n",
    "# first tensor is the hidden state, h0\n",
    "# second tensor initializes the cell memory, c0\n",
    "h0 = torch.randn(1, 1, hidden_dim)\n",
    "c0 = torch.randn(1, 1, hidden_dim)\n",
    "\n",
    "\n",
    "#h0 = Variable(h0)\n",
    "#c0 = Variable(c0)\n",
    "\n",
    "print('Initial hidden state', h0)\n",
    "print('Initial cell memory', c0)\n",
    "print()\n",
    "# step through the sequence one element at a time.\n",
    "for i in inputs_list:\n",
    "    # wrap in Variable \n",
    "    #i = Variable(i)\n",
    "    \n",
    "    # after each step, hidden contains the hidden state\n",
    "    out, hidden = lstm(i.view(1, 1, -1), (h0, c0))\n",
    "    print('out: \\n', out)\n",
    "    print('hidden: \\n', hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we have `5` sequences, each of size `4`. Per **each sequence** we produce `4` outputs, each size of 3 (because `hidden_dim` is 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide to LSTM in Pytorch - define the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resource**: https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step first**:<br>\n",
    "\n",
    "Recall, how `nn.LSTM` module is defined for a simple case:<br>\n",
    "- We have a sequence (whole training dataset as a time-series), each sample is `4` elements.<br>\n",
    "- Features = `hidden_dim` (`hidden_dim = 3`). Output generates `3` values. <br>\n",
    "- Input has a shape of `(seq_len, batch, input_size)`. `seq_len` - number of timesteps or simply saying, how many elements we have in one sequence or `t` (`T_x`). <br>\n",
    "`batch` - do we feed the whole dataset at once (`m=1`) or break it into mini-batches (`m` is any number).<br>\n",
    "`input_size` - the shape of element at time `t`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step second**:\n",
    "\n",
    "Next we initialize hidden state (which is STM or output) `h0` and cell state (which is LTM) `c0`.<br>\n",
    "We randomply initialize them (`torch.randn`) with the size of h0 and c0 to be **(1,1 `hidden_dim`)**.<br>\n",
    "\n",
    "\n",
    "Using `Variable` is an optional step for the last Pytorch versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "h0 = torch.randn(1, 1, hidden_dim)\n",
    "c0 = torch.randn(1, 1, hidden_dim)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What's the shape of `h0`and `c0`?** <br> <br>\n",
    " It's `(num_layers * num_directions, batch, hidden_size)` for both `h0`and `c0`.<br>\n",
    " In our case: `num_layers` and `num_directions` is 1 (LSTM is uniderectional). We feed inputs one by one, therefore `batch_size` = 1. The size of hidden state is `hidden_size` = `hidden_dim` = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third step**:<br>\n",
    "\n",
    "For all elements in taining data sequence, we feed them one by one to `nn.LSTM`. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments, Inputs, Outputs\n",
    "#### Additional source to read: https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on **arguments** and **inputs** of `nn.LSTM`.<br>\n",
    "**Arguments**: \n",
    "\n",
    "> - `input_size` (in our case it is `input_dim = 4`). In some other sources (e.g. Andrew Ng Deep learning course) it's named `n_x`.\n",
    "- `hidden_size` - number of features in hidden state (which is output or STM). In our case it is `hidden_dim = 3`. The output will be of this size;\n",
    "- `num_layers`. In our case it's 1 by default.  \n",
    "This is number of recurrent layers. E.g., setting ``num_layers=2`` would mean stacking two LSTMs together to form a `stacked LSTM`, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "- `bias` - Default: ``True``.\n",
    "- `batch_first`: If ``True``, then the input and output tensors are provided as `(batch, seq, feature)`. Default: ``False``\n",
    "- `dropout` - probability of dropout. \n",
    "- `bidirectional`: If ``True``, becomes a bidirectional LSTM. Default: ``False``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs**: input, `(h_0, c_0)`\n",
    "\n",
    "> - `(h_0, c_0)` - initialized tuple of hidden state STM and cell state (LTM).<br><br>\n",
    "- **input** of shape `(seq_len, batch, input_size)`: tensor containing the features of the input sequence, where `seq_len` = `t` (number of timesteps). In Andrew Ng course `seq_len` is notated `T_x`. <br><br>\n",
    "In our case this is `i.view(1, 1, -1)`, where `i` is an input element in sequence. <br>\n",
    "Note, that if we specified `batch_first` = ``True``, our input will be of size `(batch, seq, feature)`. In Andrew Ng course `batch` is called `m`. `batch` is a batch size and called `B` sometimes. <br><br>\n",
    "In case of **input**, `feature` is `input_size`. Correspondingly, for **output** `feature` is `num_directions * hidden_size`, where `hidden_size` = `hidden_dim`.\n",
    "- `h0` - initialized hidden state of shape `(num_layers * num_directions, batch, hidden_size)`. Tensor containing the initial hidden state for each element in the batch. In Andrew Ng course `hidden_size` is `n_a`.<br><br>\n",
    "If LSTM is biderectional, `num_directions` is 2. Otherwise it's 1.\n",
    "- `c0` - initialized cell state of shape `(num_layers * num_directions, batch, hidden_size)`. Tensor\n",
    "containing the initial cell state for each element in the batch.<br><br>\n",
    "*Remark: If `(h_0, c_0)` is not provided, both `h_0` and `c_0` default to zero.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**: output, (h_n, c_n)\n",
    "\n",
    "> - `(h_n, c_n)` - tuple of hidden state STM and cell state LTM, outputed by n-th layer.<br><br>\n",
    "- **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor with output from **all timestemps** on **last layer** of network. `seq_len` is timestamps or `T_x` (in some sources).<br><br>\n",
    "- `h_n` - initialized hidden state of shape `(num_layers * num_directions, batch, hidden_size)`: tensor with output containing the hidden state from **last timestep**  on **all layers**. `seq_len` is timestamps or `T_x` (in some sources). <br><br>\n",
    "If LSTM is biderectional, `num_directions` is 2. Otherwise it's 1.\n",
    "- `c_0` - initialized cell state of shape `(num_layers * num_directions, batch, hidden_size)`. Tensor  containing the cell state  for `t = seq_len`.<br><br>\n",
    "*Remark 1: if we have multiple layers, the output `h_n` and cell state `c_n` can be separated using `h_n.view(num_layers, num_directions, batch, hidden_size)` or `c_n.view(num_layers, num_directions, batch, hidden_size)`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LSTM with batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**:  <br>\n",
    "1. https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677\n",
    "2. https://towardsdatascience.com/all-you-need-to-know-about-rnns-e514f0b00c7c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using batch with specified `batch_size` or `m`, we break whole dataset into `m` number of sequnces of specified length ``seq_len``. ``seq_len`` corresponds to the number of time periods ``t``. <br>\n",
    "Important point here is that LSTM produces the output for each element in sequence ``seq_len`` per each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider following example. We want to feed 50 elements into lstm, breaking them into 5 sequences and produce the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors = [torch.randn(10) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2380, -1.7623,  0.4873,  1.4592,  1.4165,  1.0032, -0.5644,  0.3819,\n",
      "         1.7595,  1.2146])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: ``m = 1``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with `batch_size = 1` and will be feeding sequence by sequence, generating the output at each timestep.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        # define size of input, hidden units and batch size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        # define LSTM model\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim)\n",
    "        # initialize hidden state\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(1, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(1, batch_size, hidden_dim)\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        # input should be of size (sequence_length, batch_size, input_size)\n",
    "        output, self.hidden = self.lstm(sequence.view(len(sequence),batch_size, -1), \n",
    "                                        self.hidden)\n",
    "        \n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1 # input size is 1: we will feed item of size 1 per timestamp\n",
    "hidden_dim = 5 \n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = LSTM(input_size, hidden_dim, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, prepare the sequence of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 5\n",
      "The length of one sequence: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of sequences:',len(input_tensors))\n",
    "print('The length of one sequence:',len(input_tensors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: \n",
      " tensor([[[-1.6173e-02,  2.1318e-02,  8.9537e-03,  1.3004e-01, -1.3322e-01]],\n",
      "\n",
      "        [[-2.3461e-01, -3.0454e-04,  4.3860e-02,  1.2459e-01, -1.0315e-01]],\n",
      "\n",
      "        [[-4.5394e-03,  4.6846e-02,  4.3741e-02,  1.6021e-01, -1.6202e-01]],\n",
      "\n",
      "        [[ 9.2667e-02,  8.7011e-02, -2.3487e-02,  1.7278e-01, -1.9596e-01]],\n",
      "\n",
      "        [[ 1.3717e-01,  1.0959e-01, -5.3626e-02,  2.0487e-01, -2.3011e-01]],\n",
      "\n",
      "        [[ 1.6384e-01,  1.1752e-01, -4.0300e-02,  2.4500e-01, -2.4689e-01]],\n",
      "\n",
      "        [[ 9.9148e-02,  1.0298e-01,  1.6554e-02,  3.1784e-01, -2.1384e-01]],\n",
      "\n",
      "        [[ 1.0508e-01,  9.0466e-02,  2.5842e-02,  2.7210e-01, -1.7975e-01]],\n",
      "\n",
      "        [[ 1.2585e-01,  1.0373e-01, -4.9865e-02,  2.0447e-01, -1.8706e-01]],\n",
      "\n",
      "        [[ 1.6040e-01,  1.1981e-01, -5.2092e-02,  2.4728e-01, -2.3765e-01]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[ 0.1604,  0.1198, -0.0521,  0.2473, -0.2376]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.5049,  0.4117, -0.1259,  0.9625, -0.6305]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[ 0.1733,  0.1194, -0.0381,  0.2648, -0.2382]],\n",
      "\n",
      "        [[ 0.1561,  0.1104,  0.0036,  0.3154, -0.2259]],\n",
      "\n",
      "        [[ 0.0582,  0.0861,  0.0392,  0.3275, -0.1695]],\n",
      "\n",
      "        [[-0.1569,  0.0505,  0.0643,  0.2825, -0.1008]],\n",
      "\n",
      "        [[-0.2895,  0.0146,  0.0840,  0.1830, -0.0737]],\n",
      "\n",
      "        [[-0.2647, -0.0016,  0.1029,  0.1470, -0.0954]],\n",
      "\n",
      "        [[-0.1360,  0.0133,  0.1085,  0.1742, -0.1375]],\n",
      "\n",
      "        [[ 0.0430,  0.0621,  0.0526,  0.1792, -0.1773]],\n",
      "\n",
      "        [[-0.0489,  0.0521,  0.0604,  0.2573, -0.1786]],\n",
      "\n",
      "        [[ 0.0498,  0.0670,  0.0550,  0.2316, -0.1727]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[ 0.0498,  0.0670,  0.0550,  0.2316, -0.1727]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.1247,  0.1971,  0.1488,  0.6673, -0.3692]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[-0.0043,  0.0587,  0.0617,  0.2778, -0.1690]],\n",
      "\n",
      "        [[ 0.0922,  0.0817,  0.0228,  0.2196, -0.1764]],\n",
      "\n",
      "        [[ 0.1334,  0.0971, -0.0043,  0.2466, -0.2134]],\n",
      "\n",
      "        [[ 0.1383,  0.0976,  0.0113,  0.2876, -0.2169]],\n",
      "\n",
      "        [[-0.1705,  0.0630,  0.0453,  0.2393, -0.1028]],\n",
      "\n",
      "        [[-0.0439,  0.0544,  0.0662,  0.2164, -0.1322]],\n",
      "\n",
      "        [[ 0.0181,  0.0610,  0.0579,  0.2379, -0.1646]],\n",
      "\n",
      "        [[ 0.1016,  0.0900, -0.0129,  0.1971, -0.1840]],\n",
      "\n",
      "        [[ 0.0703,  0.0897,  0.0236,  0.2988, -0.2101]],\n",
      "\n",
      "        [[ 0.1001,  0.0866,  0.0268,  0.2612, -0.1833]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[ 0.1001,  0.0866,  0.0268,  0.2612, -0.1833]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.2493,  0.2651,  0.0714,  0.7897, -0.3987]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[ 0.0911,  0.0835,  0.0348,  0.2898, -0.1884]],\n",
      "\n",
      "        [[-0.1822,  0.0489,  0.0581,  0.2521, -0.1017]],\n",
      "\n",
      "        [[-0.2691,  0.0149,  0.0831,  0.1690, -0.0811]],\n",
      "\n",
      "        [[-0.1048,  0.0272,  0.0946,  0.1844, -0.1340]],\n",
      "\n",
      "        [[-0.0964,  0.0281,  0.0907,  0.2191, -0.1493]],\n",
      "\n",
      "        [[ 0.0398,  0.0592,  0.0636,  0.2136, -0.1739]],\n",
      "\n",
      "        [[ 0.0859,  0.0737,  0.0407,  0.2489, -0.1989]],\n",
      "\n",
      "        [[ 0.0332,  0.0662,  0.0542,  0.2904, -0.1801]],\n",
      "\n",
      "        [[ 0.0669,  0.0705,  0.0525,  0.2661, -0.1720]],\n",
      "\n",
      "        [[ 0.0313,  0.0647,  0.0586,  0.2925, -0.1683]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[ 0.0313,  0.0647,  0.0586,  0.2925, -0.1683]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.0638,  0.1711,  0.1799,  0.7134, -0.2993]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[ 0.0234,  0.0610,  0.0648,  0.2846, -0.1588]],\n",
      "\n",
      "        [[ 0.0550,  0.0663,  0.0585,  0.2735, -0.1675]],\n",
      "\n",
      "        [[ 0.1093,  0.0833,  0.0232,  0.2462, -0.1857]],\n",
      "\n",
      "        [[ 0.1153,  0.0870,  0.0269,  0.2906, -0.2033]],\n",
      "\n",
      "        [[ 0.0230,  0.0703,  0.0515,  0.3141, -0.1640]],\n",
      "\n",
      "        [[ 0.0732,  0.0746,  0.0473,  0.2658, -0.1650]],\n",
      "\n",
      "        [[ 0.0886,  0.0780,  0.0406,  0.2816, -0.1826]],\n",
      "\n",
      "        [[ 0.0481,  0.0702,  0.0525,  0.3028, -0.1704]],\n",
      "\n",
      "        [[-0.0506,  0.0524,  0.0692,  0.2981, -0.1373]],\n",
      "\n",
      "        [[-0.1329,  0.0340,  0.0840,  0.2673, -0.1152]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.1329,  0.0340,  0.0840,  0.2673, -0.1152]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[-0.2472,  0.0801,  0.3012,  0.5363, -0.1806]]],\n",
      "       grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "for sequence in input_tensors:\n",
    "    # after each step, hidden contains the hidden state\n",
    "    out, hidden = model(sequence)\n",
    "    print('out: \\n', out)\n",
    "    print('hidden: \\n', hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of output from a single sequence for m = 1: (10, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of output from a single sequence for m = 1:', tuple(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0234,  0.0610,  0.0648,  0.2846, -0.1588],\n",
       "        [ 0.0550,  0.0663,  0.0585,  0.2735, -0.1675],\n",
       "        [ 0.1093,  0.0833,  0.0232,  0.2462, -0.1857],\n",
       "        [ 0.1153,  0.0870,  0.0269,  0.2906, -0.2033],\n",
       "        [ 0.0230,  0.0703,  0.0515,  0.3141, -0.1640],\n",
       "        [ 0.0732,  0.0746,  0.0473,  0.2658, -0.1650],\n",
       "        [ 0.0886,  0.0780,  0.0406,  0.2816, -0.1826],\n",
       "        [ 0.0481,  0.0702,  0.0525,  0.3028, -0.1704],\n",
       "        [-0.0506,  0.0524,  0.0692,  0.2981, -0.1373],\n",
       "        [-0.1329,  0.0340,  0.0840,  0.2673, -0.1152]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.view(10, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.view(10,1, -1)[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: mini-batching  ``m = 5``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the batch size and check the output of model with `m = 5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1 # input size is 1: we will feed item of size 1 per timestamp\n",
    "hidden_dim = 5 \n",
    "batch_size = 1\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_batch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, batch_size, seq_len):\n",
    "        super(LSTM_batch, self).__init__()\n",
    "        # define size of input, hidden units and batch size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len = seq_len\n",
    "        # define LSTM model\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, batch_first = True)\n",
    "        # initialize hidden state\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(1, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros(1, batch_size, hidden_dim)\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        seq_len = int(len(sequence)/batch_size)\n",
    "        # input should be of size (sequence_length, batch_size, input_size)\n",
    "        output, self.hidden = self.lstm(sequence.view(batch_size, seq_len, -1), \n",
    "                                        self.hidden)\n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model_batch = LSTM_batch(input_size, hidden_dim, batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: \n",
      " tensor([[[-0.0074, -0.0056,  0.0566,  0.1231,  0.0049],\n",
      "         [ 0.0064, -0.0015,  0.0713,  0.1677, -0.0076],\n",
      "         [-0.0181,  0.0030,  0.1053,  0.2300, -0.0222],\n",
      "         [-0.0247,  0.0065,  0.1203,  0.2518, -0.0426],\n",
      "         [-0.0987,  0.0076,  0.1979,  0.4096, -0.0521],\n",
      "         [-0.1307,  0.0129,  0.2494,  0.4450, -0.0879],\n",
      "         [-0.0488,  0.0133,  0.1891,  0.2970, -0.1414],\n",
      "         [-0.0234,  0.0080,  0.1256,  0.2759, -0.1338],\n",
      "         [ 0.0525,  0.0050,  0.0445,  0.1847, -0.1274],\n",
      "         [-0.0316,  0.0010,  0.1157,  0.2979, -0.0894]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "hidden: \n",
      " (tensor([[[-0.0316,  0.0010,  0.1157,  0.2979, -0.0894]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[-0.0809,  0.0014,  0.2184,  0.7585, -0.2072]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[ 1.1231e-01,  3.3039e-03, -4.5702e-03,  1.1615e-01, -1.0374e-01],\n",
      "         [ 1.4703e-03,  1.7051e-03,  8.5396e-02,  2.4820e-01, -5.9641e-02],\n",
      "         [-4.2029e-02,  4.8467e-03,  1.3734e-01,  2.9322e-01, -7.0458e-02],\n",
      "         [ 2.4271e-02,  6.3853e-03,  8.2899e-02,  2.0440e-01, -9.1320e-02],\n",
      "         [-1.2051e-02,  4.9720e-03,  1.0087e-01,  2.5708e-01, -7.8375e-02],\n",
      "         [-1.2204e-02,  5.3053e-03,  1.0518e-01,  2.5169e-01, -8.5884e-02],\n",
      "         [-5.2352e-02,  4.6296e-03,  1.4322e-01,  3.1351e-01, -8.4991e-02],\n",
      "         [ 3.0045e-02,  5.7524e-03,  7.8518e-02,  2.0144e-01, -1.0666e-01],\n",
      "         [ 8.8180e-02,  4.5273e-03, -3.0355e-04,  1.4139e-01, -8.7989e-02],\n",
      "         [ 8.3712e-02,  3.7282e-03, -1.0040e-02,  1.4052e-01, -6.1409e-02]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "hidden: \n",
      " (tensor([[[ 0.0837,  0.0037, -0.0100,  0.1405, -0.0614]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.2110,  0.0058, -0.0162,  0.4643, -0.1016]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[-0.0324,  0.0030,  0.1205,  0.3052, -0.0429],\n",
      "         [-0.0127,  0.0077,  0.1218,  0.2461, -0.0735],\n",
      "         [-0.0126,  0.0073,  0.1093,  0.2526, -0.0793],\n",
      "         [ 0.0063,  0.0066,  0.0884,  0.2294, -0.0854],\n",
      "         [ 0.0365,  0.0057,  0.0541,  0.1948, -0.0837],\n",
      "         [ 0.0336,  0.0048,  0.0506,  0.1980, -0.0735],\n",
      "         [-0.0077,  0.0044,  0.0924,  0.2496, -0.0670],\n",
      "         [-0.0383,  0.0053,  0.1297,  0.2887, -0.0748],\n",
      "         [ 0.0838,  0.0061,  0.0246,  0.1438, -0.0907],\n",
      "         [ 0.0952,  0.0051, -0.0165,  0.1304, -0.0625]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "hidden: \n",
      " (tensor([[[ 0.0952,  0.0051, -0.0165,  0.1304, -0.0625]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.2387,  0.0082, -0.0262,  0.4467, -0.1000]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[-0.0112,  0.0041,  0.0960,  0.2649, -0.0425],\n",
      "         [ 0.0744,  0.0067,  0.0297,  0.1476, -0.0634],\n",
      "         [-0.0468,  0.0055,  0.1452,  0.3407, -0.0434],\n",
      "         [-0.0546,  0.0106,  0.1711,  0.3072, -0.0790],\n",
      "         [ 0.0756,  0.0095,  0.0456,  0.1553, -0.0979],\n",
      "         [ 0.0419,  0.0070,  0.0440,  0.1923, -0.0697],\n",
      "         [ 0.0540,  0.0062,  0.0299,  0.1725, -0.0651],\n",
      "         [ 0.0911,  0.0057, -0.0122,  0.1318, -0.0561],\n",
      "         [ 0.0540,  0.0054,  0.0221,  0.1701, -0.0411],\n",
      "         [ 0.0488,  0.0061,  0.0323,  0.1739, -0.0417]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "hidden: \n",
      " (tensor([[[ 0.0488,  0.0061,  0.0323,  0.1739, -0.0417]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.1237,  0.0092,  0.0536,  0.5303, -0.0739]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "out: \n",
      " tensor([[[ 0.0886,  0.0065, -0.0077,  0.1319, -0.0417],\n",
      "         [ 0.1352,  0.0065, -0.0690,  0.0845, -0.0299],\n",
      "         [ 0.0541,  0.0065,  0.0195,  0.1663, -0.0150],\n",
      "         [ 0.0622,  0.0078,  0.0197,  0.1549, -0.0238],\n",
      "         [ 0.1397,  0.0077, -0.0673,  0.0795, -0.0236],\n",
      "         [ 0.0535,  0.0079,  0.0227,  0.1672, -0.0104],\n",
      "         [ 0.0626,  0.0088,  0.0207,  0.1545, -0.0209],\n",
      "         [-0.0231,  0.0090,  0.1126,  0.2764, -0.0254],\n",
      "         [-0.0258,  0.0110,  0.1292,  0.2648, -0.0530],\n",
      "         [-0.0011,  0.0104,  0.1024,  0.2364, -0.0704]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "hidden: \n",
      " (tensor([[[-0.0011,  0.0104,  0.1024,  0.2364, -0.0704]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[-0.0027,  0.0152,  0.1717,  0.6793, -0.1338]]],\n",
      "       grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "for sequence in input_tensors:\n",
    "    # after each step, hidden contains the hidden state\n",
    "    out, hidden = model_batch(sequence)\n",
    "    print('out: \\n', out)\n",
    "    print('hidden: \\n', hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of output from a single sequence for m = 5: (1, 10, 5)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of output from a single sequence for m = 5:', tuple(out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aditional notes about input/output shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input**<br>\n",
    "\n",
    "The input can also be a **packed variable length sequence**.<br>\n",
    "See functions: `torch.nn.utils.rnn.pack_padded_sequence` or `torch.nn.utils.rnn.pack_sequence` for details.<br>\n",
    "\n",
    "*Reference: \n",
    "https://github.com/HarshTrivedi/packing-unpacking-pytorch-minimal-tutorial\n",
    "https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pack_padded_sequence.html*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
